{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Prize challenges with judges reviewing them can be unwieldy sucks on manpower. This code is designed to take a *very* simple case of keyword selection and develop similarity scores between judges with specific expertise and submission that require that expertise. \n",
    "\n",
    "Effectively, we use simple keyword vectors for applications (chosen by the applicants themselves from a limited dropdown list) and use those same keywords to describe the expertise of a group of external judges (whose expertise keywords are chosen by hand by prize staff). By measuring the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between these two groups, we can then determine a similarity score that helps speed up the process of matching prize judges with applications that are most relevent to their field of knowledge.\n",
    "\n",
    "# The Data\n",
    "\n",
    "The data intended for input herein are prize submission metadata from herox.com, a prizes and challenges hosting platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE CONTROL CENTER\n",
    "Constants for controlling the assignment behavior for submissions are below this line, please modify these as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGES_PER_APP = 5\n",
    "MAX_REVIEWS_PER_JUDGE = 18\n",
    "\n",
    "#How many special/flagged judges should there be per app?\n",
    "FLAGGED_JUDGES_PER_APP = 1\n",
    "\n",
    "OUTPUT_FILEPATH = 'Data/Output_Files/American-Made_0101_autoAssigned_5Judge_18cap.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "from Submission import Submission\n",
    "from Judge import Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the submissions data\n",
    "\n",
    "Want starting output to be submission IDs as column names and keywords as index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Keyword data for prize submissions/applications\n",
    "#column number 255: Unique ID column\n",
    "s = pd.read_csv('Data/0101HeroXSubs.csv', index_col = 255, \n",
    "                encoding = 'latin1')\n",
    "\n",
    "s = s.loc[:,'Keyword tags'].dropna(how = 'all')\n",
    "\n",
    "s = s.str.split('\\r', expand = True).transpose()\n",
    "s.index = ['Keyword ' + str(x) for x in range (1,len(s)+1)]\n",
    "s.index.name, s.columns.name = ('Keywords', 'Submissions')\n",
    "\n",
    "\n",
    "s = pd.melt(s).dropna()\n",
    "\n",
    "s.rename(columns = {'value': 'keyword'}, inplace = True)\n",
    "s['value'] = 1\n",
    "s = s.pivot_table(index = 'keyword', columns = 'Submissions', values = 'value').fillna(0).astype('int')\n",
    "s = s.transpose()\n",
    "\n",
    "s.drop(columns = ['Other or N/A'], inplace = True)\n",
    "\n",
    "#Make sure the columns are sorted in the same order for submissions as they are for judges\n",
    "s = s.reindex(columns = sorted(s.columns))\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#How many projects have no keywords?\n",
    "s[s.sum(axis = 1) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the judges data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = pd.read_csv('Data/0101JudgePanel.csv', index_col = 'Name',\n",
    "                dtype = 'str').transpose().dropna(how = 'all', axis =1)\n",
    "\n",
    "j.columns.name = 'Judge'\n",
    "j.drop(['Status', 'Title', 'Company', 'Email', 'Notes', 'Garrett Notes'], inplace = True)\n",
    "\n",
    "j = pd.melt(j).dropna()\n",
    "\n",
    "j.rename(columns = {'value': 'keyword'}, inplace = True)\n",
    "j['value'] = 1\n",
    "\n",
    "j = j.pivot_table(index = 'keyword', columns = 'Judge', values = 'value').fillna(0).astype('int')\n",
    "j = j.transpose()\n",
    "\n",
    "#Not worrying about Business keyword yet\n",
    "j_matched = j.drop(columns = ['Business'])\n",
    "\n",
    "#Make sure the columns are sorted in the same order for judges as they are for submissions\n",
    "j_matched = j_matched.reindex(index = sorted(j_matched.index))\n",
    "\n",
    "j_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to make sure the keyword pools for both submissions and judges match\n",
    "\n",
    "**NOTE: ** often there are keyword misspellings or extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Are the column counts the same??\n",
    "(s.columns == j_matched.columns).sum() == len(s.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Judge and Submission objects; Run the cosine similarity scoring and assign Judges to Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import assignments\n",
    "\n",
    "#Run the assignments 100 times and average over the assignment count for each judge to see \n",
    "    #if number of keywords per judge dictates # of assignments\n",
    "for i in range(0,100):\n",
    "    #Make the flagged Judges\n",
    "    judges = [Judge(name, MAX_REVIEWS_PER_JUDGE, flag = True) for name in j[j['Business'] != 0].index]\n",
    "\n",
    "    #Now add in the unflagged Judges\n",
    "    judges += [Judge(name, MAX_REVIEWS_PER_JUDGE, flag = False) for name in j[j['Business'] == 0].index]\n",
    "    \n",
    "    #Create all the Submissions\n",
    "    submissions = [Submission(name, JUDGES_PER_APP, FLAGGED_JUDGES_PER_APP) for name in s.index]\n",
    "    \n",
    "    \n",
    "    scores = assignments.similarity_scoring(s, j_matched, JUDGES_PER_APP, random_seed = None)\n",
    "    assignments.make_assignments(scores, submissions, judges)\n",
    "    \n",
    "    judge_data = pd.DataFrame(data = None)\n",
    "    for e in judges:\n",
    "        judge_data = judge_data.append(e.to_df())\n",
    "        \n",
    "avg_judge_data = judge_data.groupby('Judge Name').mean()\n",
    "avg_judge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_judge_data['Keyword Count'] = j_matched.transpose().sum()\n",
    "avg_judge_data.reset_index(inplace = True)\n",
    "avg_judge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "#sns.scatterplot(x = 'Keyword Count', y = 'Number of Assignments', data = avg_judge_data)\n",
    "sns.regplot(x = 'Keyword Count', y = 'Number of Assignments', data = avg_judge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(avg_judge_data.corr().loc['Number of Assignments', 'Keyword Count'], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like our hypothesis is unfounded. Higher keyword counts don't seem to correlate with lower assignment numbers after all,** when a number (100 in this case) of random trials are averaged over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to make sure we don't have any apps with assignment violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the optimized assignments to a DataFrame and export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['Judge ' + str(x) for x in range(1,JUDGES_PER_APP + 1)]\n",
    "\n",
    "output = pd.DataFrame(data = None, columns = cols, index = [x.id for x in submissions])\n",
    "\n",
    "for sub in submissions:\n",
    "    output.loc[sub.id] = [x.name for x in sub.assigned_judges]\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(OUTPUT_FILEPATH)\n",
    "#app_assignments.to_csv('Data/Output_Files/American-Made_Solar_Prize_App_Assignments_test_manualAssignments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "1. Allow randomness of judges to be stochastic (no random seed), run X times and plot Judge keyword count vs. average submission count\n",
    "    * Rachelle worried that she's seeing a trend where higher keyword count = lower assignment load\n",
    "3. Figure out if we can parse a single cell list of keywords that are separated by \\n into keywords as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SolarPrize]",
   "language": "python",
   "name": "conda-env-SolarPrize-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
